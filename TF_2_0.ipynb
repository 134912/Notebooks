{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF_2.0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zaidalyafeai/Notebooks/blob/master/TF_2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "M7r1fJQniHj7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://www.gstatic.com/devrel-devsite/vb90cdc5a0568bddc672448d83f2c35981bb3eb615c941c833b28e75d6cd03766/tensorflow/images/rebrand/lockup.svg)"
      ]
    },
    {
      "metadata": {
        "id": "q5EbZRSRU7X-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The new version of TensorFlow is changing the way to code machine learning models to be more pythonic. Debugging and creating models were complicated with sessions and graphs but not anymore. Eager Execution allows evaluating tensors on the fly without having to run a session to execute code from end to end. Now, TensorFlow 2.0 introduces eager execution by default.  "
      ]
    },
    {
      "metadata": {
        "id": "SQLWlBkaUJwJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Install TF-2.0 alpha\n",
        "\n",
        "As of the time I created this tutorial TF 2.0 is still in the early alpha phase. It is not integrated with colab yet so you have to pip install it first "
      ]
    },
    {
      "metadata": {
        "id": "qLOdEd07UI-x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "c55a4366-8bdd-42a8-a07f-54296d340664"
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0-alpha0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/66/32cffad095253219d53f6b6c2a436637bbe45ac4e7be0244557210dc3918/tensorflow_gpu-2.0.0a0-cp36-cp36m-manylinux1_x86_64.whl (332.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 332.1MB 55kB/s \n",
            "\u001b[?25hCollecting google-pasta>=0.1.2 (from tensorflow-gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/96/adbd4eafe72ce9b5ca6f168fbf109386e1b601f7c59926a11e9d7b7a5b44/google_pasta-0.1.4-py3-none-any.whl (51kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 23.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.6.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.9)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.11.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.7)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 (from tensorflow-gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/82/f16063b4eed210dc2ab057930ac1da4fbe1e91b7b051a6c8370b401e6ae7/tf_estimator_nightly-1.14.0.dev2019030115-py2.py3-none-any.whl (411kB)\n",
            "\u001b[K    100% |████████████████████████████████| 419kB 12.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.2)\n",
            "Collecting tb-nightly<1.14.0a20190302,>=1.14.0a20190301 (from tensorflow-gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/51/aa1d756644bf4624c03844115e4ac4058eff77acd786b26315f051a4b195/tb_nightly-1.14.0a20190301-py3-none-any.whl (3.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.0MB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.33.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (40.8.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.0.1)\n",
            "Installing collected packages: google-pasta, tf-estimator-nightly, tb-nightly, tensorflow-gpu\n",
            "Successfully installed google-pasta-0.1.4 tb-nightly-1.14.0a20190301 tensorflow-gpu-2.0.0a0 tf-estimator-nightly-1.14.0.dev2019030115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CiwZoGf85dnU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import tensorflow"
      ]
    },
    {
      "metadata": {
        "id": "jlnQG8hC-uCg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Fq5Gb135Z5f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Check the version of tf"
      ]
    },
    {
      "metadata": {
        "id": "MVq8s_1cUVNA",
        "colab_type": "code",
        "outputId": "02689104-7beb-4e9d-edaa-39558ddf50a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0-alpha0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "UC6X5Y844_E-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Executing Ops Eagerly \n",
        "By perfoming operations you can see the output directly without creating a session. "
      ]
    },
    {
      "metadata": {
        "id": "DmwZJKlA_B15",
        "colab_type": "code",
        "outputId": "fbb0895c-9963-4de9-edee-b8b06e8b96a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x = [[2.]]\n",
        "m = tf.square(x)\n",
        "print(m)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[4.]], shape=(1, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WGLoIPrQ6ZYT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can call `.numpy` to retrieve the results of the tensor as a numpy array"
      ]
    },
    {
      "metadata": {
        "id": "-FGFGbZq6fRo",
        "colab_type": "code",
        "outputId": "bfee1d46-e606-4d38-b152-26e0ba10114c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "m.numpy()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "l9Xlu7in6m22",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can also compute an operation including two tensors "
      ]
    },
    {
      "metadata": {
        "id": "d4tKJJ90_QMM",
        "colab_type": "code",
        "outputId": "8c8587fc-d884-450a-d33e-445427661c45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "a = tf.constant([[1, 2],\n",
        "                 [3, 4]])\n",
        "\n",
        "b = tf.constant([[2, 1],\n",
        "                 [3, 4]])\n",
        "\n",
        "ab = tf.matmul(a, b)\n",
        "\n",
        "print('a * b = \\n', ab.numpy())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a * b = \n",
            " [[ 8  9]\n",
            " [18 19]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V5qlVJygETcb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Constants and Variables \n",
        "\n",
        "\n",
        "*   `tf.constant`, creates a constant tensor populated with the values as argument. The values are immutable. \n",
        "*   `tf.Variable `, this method encapsultes a mutable tensor that can be changed later using assign \n"
      ]
    },
    {
      "metadata": {
        "id": "ayMVXFj1FZxz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Creating a constant tensor "
      ]
    },
    {
      "metadata": {
        "id": "g2KFQKSLFNlS",
        "colab_type": "code",
        "outputId": "f0fdae42-652f-45b1-f8fd-3e19f1b74305",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "a = tf.constant([[2,3]])\n",
        "print(a)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[2 3]], shape=(1, 2), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mn8uX4t5FtHp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A constant tensor is immutable so you cannot assign a new value to it."
      ]
    },
    {
      "metadata": {
        "id": "xrJqeZfgHU-j",
        "colab_type": "code",
        "outputId": "a84bb8f8-0cbf-4f90-fa1a-d2988f9753e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "  a.assign(2.)\n",
        "except:\n",
        "  print('Exception raised ')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exception raised \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lrgbhCu8H3rm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "On the other hand variables are mutable and can be assigned a new value"
      ]
    },
    {
      "metadata": {
        "id": "WSMIotOQFw2F",
        "colab_type": "code",
        "outputId": "23a38527-708a-4d7b-eadc-bd8259d60591",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "v = tf.Variable(5.)\n",
        "\n",
        "print('Old value for v =', v.numpy())\n",
        "v.assign(2.)\n",
        "print('New value for v =', v.numpy())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Old value for v = 5.0\n",
            "New value for v = 2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "se0MFrEwMXWo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can also in-place increment/decrement the value of a tensor "
      ]
    },
    {
      "metadata": {
        "id": "d9M50PpdMzEn",
        "colab_type": "code",
        "outputId": "b6c604bc-e064-4fdf-9320-fc7919d4e8de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "v.assign(2.)\n",
        "print('value     : ', v.numpy())\n",
        "print('increment : ', v.assign_add(1.).numpy())\n",
        "print('decrement : ', v.assign_sub(1.).numpy())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "value     :  2.0\n",
            "increment :  3.0\n",
            "decrement :  2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FtGAYUUWI8bX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can return many information from a tensor variable, like the name, type , shape and what device it executes on. "
      ]
    },
    {
      "metadata": {
        "id": "2wQMtx3QJBSg",
        "colab_type": "code",
        "outputId": "01f6ea07-99b6-4155-d74f-c903e59b2d2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "print('name  : ', v.name)\n",
        "print('type  : ', v.dtype)\n",
        "print('shape : ', v.shape)\n",
        "print('device: ', v.device)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "name  :  Variable:0\n",
            "type  :  <dtype: 'float32'>\n",
            "shape :  ()\n",
            "device:  /job:localhost/replica:0/task:0/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5irSm-yDN0nV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Gradient Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "y36ig_TVAAoM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Gradient evaluation is very importnat machine learning because it is based on function optimization. You can use `tf.GradientTape()` method to record the gradient of an arbitrary function"
      ]
    },
    {
      "metadata": {
        "id": "bdZmXyAi_3M3",
        "colab_type": "code",
        "outputId": "e01e9e6b-09db-4c39-bb39-1fb7e1c2fb16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "w = tf.Variable(2.0)\n",
        "\n",
        "#watch the gradient of the loss operation\n",
        "with tf.GradientTape() as tape:\n",
        "  loss = w * w\n",
        "\n",
        "grad = tape.gradient(loss, w)\n",
        "print(f'The gradient of w^2 at {w.numpy()} is {grad.numpy()}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The gradient of w^2 at 2.0 is 4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "0NhC1mQbao1R"
      },
      "cell_type": "markdown",
      "source": [
        " In this example we evaluate the gradient of the sigmoid function \n",
        "\n",
        "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
        "\n",
        "Note that \n",
        "\n",
        "$$\\sigma'(x) = \\frac{e^{-x}}{(1+e^{-x})^2} = \\sigma(x)(1-\\sigma(x)) $$\n",
        "\n",
        "For isntance \n",
        "\n",
        "$$\\sigma'(0) = \\sigma(0)(1-\\sigma(0)) = \\frac{1}{2}\\left(1-\\frac{1}{2} \\right) = \\frac{1}{4}$$"
      ]
    },
    {
      "metadata": {
        "id": "sXH9Qjmgayf2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1 + tf.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "baf0fbf3-2142-45e8-d60b-f2da212eca82",
        "id": "tgfIrcX7YyGF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#define a varaible\n",
        "x = tf.Variable(0.)\n",
        "\n",
        "#record the gradient\n",
        "with tf.GradientTape() as tape:\n",
        "  y = sigmoid(x)\n",
        "  \n",
        "print('The gradient of the sigmoid function at 0.0 is ', tape.gradient(y, x).numpy())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The gradient of the sigmoid function at 0.0 is  0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Jew_BsZaYeVz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can also compute higher order derivatives by nesting a gradient functions. For instance, \n",
        "\n",
        "$$f(x) = \\log(x) , f'(x) = \\frac{1}{x}, f''(x) = \\frac{-1}{x^2}$$"
      ]
    },
    {
      "metadata": {
        "id": "EeLUjBdVbsKM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def log(x):\n",
        "  return tf.math.log(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hvpN3Eg0dWd-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We could compute the gradient by nesting multiple gradient tapes "
      ]
    },
    {
      "metadata": {
        "id": "UoFFIr_AXUnm",
        "colab_type": "code",
        "outputId": "3bc2e389-2a77-4659-8cb4-44c163cadd4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "#define a variable \n",
        "x = tf.Variable(1.)\n",
        "\n",
        "#record second order gradient \n",
        "with tf.GradientTape() as t1:\n",
        "  with tf.GradientTape() as t2:\n",
        "    y = log(x)\n",
        "  dx = t2.gradient(y, x)  \n",
        "dx2 = t1.gradient(dx, x)\n",
        "\n",
        "print('The first  derivative of log at x = 1 is ', dx.numpy())\n",
        "print('The second derivative of log at x = 1 is ', dx2.numpy())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The first  derivative of log at x = 1 is  1.0\n",
            "The second derivative of log at x = 1 is  -1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hBVnUE6DRDFw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Custom Gradients\n",
        "\n",
        "Some times the gradient is not what we want espeically if there is a problem in numerical instabilitiy. Consider the following function and its gradient \n",
        "\n",
        "$$f(x) = \\log(1+e^x)$$\n",
        "\n",
        "The gradient is \n",
        "\n",
        "$$f'(x) = \\frac{e^x}{1+e^x}$$\n",
        "\n",
        "Note that at big values of $x$ the gradient value will blow up."
      ]
    },
    {
      "metadata": {
        "id": "YR_7vj1TdnFS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def logexp(x):\n",
        "  return tf.math.log(1 + tf.exp(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hJtTtTQXdpqB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def grad_logexp(x):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y = logexp(x)\n",
        "  dx = tape.gradient(y, x)\n",
        "  return dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "autHEivlRp9M",
        "colab_type": "code",
        "outputId": "196ebcf6-cb02-484e-88f2-b3c95d578357",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "x = tf.Variable(0.)\n",
        "print('The gradient at x = 0  is ', grad_logexp(x).numpy())\n",
        "x = tf.Variable(100.)\n",
        "print('The gradient at x = 100 is ', grad_logexp(x).numpy()) "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The gradient at x = 0  is  0.5\n",
            "The gradient at x = 100 is  nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c19m7XawUR1a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " We can revaluate the gradient by overriding the gradient of the function. We can recompute the gradient as \n",
        "\n",
        "$$f(x) =  \\frac{1+e^x -e^x }{1+e^x} = 1 - \\frac{1}{1 + e^{x}}$$"
      ]
    },
    {
      "metadata": {
        "id": "6NBcha9HgMeM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "@tf.custom_gradient\n",
        "def logexp_stable(x):\n",
        "  e = tf.exp(x)\n",
        "  #dy is optional, allows computation of vector jacobian products for vectors other than the vector of ones.\n",
        "  def grad(dy):\n",
        "    return dy * (1 - 1 / (1 + e))\n",
        "  return tf.math.log(1 + e), grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GgrDtMCkgKM-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def grad_logexp(x):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y = logexp_stable(x)\n",
        "  dx = tape.gradient(y, x)\n",
        "  return dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EmPT2S6XUJ8C",
        "colab_type": "code",
        "outputId": "e1392387-162a-4bd0-ac8b-b3e76efd1ada",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x = tf.Variable(100.)\n",
        "print('The gradient at x = 100 is ', grad_logexp(x).numpy()) "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The gradient at x = 100 is  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qofbAYzRO3IW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Object Oriented Metrics\n",
        "You can use `metrics` to record tensors/values and operate on them at the end. This is useful when recording the training history and you want to evaluate it at the end. Use `.result()` to evaluate the metric at the end. "
      ]
    },
    {
      "metadata": {
        "id": "u4DkG8BBO74o",
        "colab_type": "code",
        "outputId": "8e5fe2e2-5b97-43fa-c9be-05a66feb095c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "m = tf.keras.metrics.Mean(\"loss\")\n",
        "\n",
        "#record the loss \n",
        "m(2)\n",
        "m(4)\n",
        "\n",
        "print('The mean loss is ', m.result().numpy())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The mean loss is  3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "erOTyJXWbHNJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you want to remove the recorded values, you can `reset` the variables "
      ]
    },
    {
      "metadata": {
        "id": "VJ20YHw9bILA",
        "colab_type": "code",
        "outputId": "4deb8ef1-7f1d-49ce-886e-caa541902d83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "m.reset_states()\n",
        "print('The mean loss is ', m.result().numpy())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The mean loss is  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hbIHKtEEbujP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Linear Regression \n",
        "\n",
        "This example is refactored from https://www.tensorflow.org/guide/eager. We create a complete example of using linear regression to predict the paramters of the function \n",
        "\n",
        "$$f(x) = 3 x + 2 + noise$$\n",
        "\n",
        "Given a point $x$ we want to predict the value of $y$. We train the model on 1000 data pairs $(x,f(x))$. \n",
        "\n",
        "The model to learn is a linear model \n",
        "\n",
        "$$\\hat{y} = W x + b$$\n",
        "\n",
        "Note that, we use `tf.GradientTape` to record the gradient with respect our trainable paramters.  \n",
        "\n",
        "We MSE to calcuate the loss \n",
        "\n",
        "$$g = (y-\\hat{y})^2$$\n",
        "\n",
        "We use Gradient Descent to update the paramters \n",
        "\n",
        "$$W = W - \\alpha  \\frac{\\partial g}{\\partial W}$$\n",
        "\n",
        "$$b = b - \\alpha  \\frac{\\partial g}{\\partial b}$$"
      ]
    },
    {
      "metadata": {
        "id": "P7A5lcylAERT",
        "colab_type": "code",
        "outputId": "0edc4740-97e2-47c6-8708-54c12f08dfd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "cell_type": "code",
      "source": [
        "#1000 data points \n",
        "NUM_EXAMPLES = 1000\n",
        "\n",
        "#define inputs and outputs with some noise \n",
        "X = tf.random.normal([NUM_EXAMPLES])  #inputs \n",
        "noise = tf.random.normal([NUM_EXAMPLES]) #noise \n",
        "y = X * 3 + 2 + noise  #true output\n",
        "\n",
        "#create model paramters with initial values \n",
        "W = tf.Variable(0.)\n",
        "b = tf.Variable(0.)\n",
        "\n",
        "#training info\n",
        "train_steps = 200\n",
        "learning_rate = 0.01\n",
        "\n",
        "for i in range(train_steps):\n",
        "  \n",
        "  #watch the gradient flow \n",
        "  with tf.GradientTape() as tape:\n",
        "    \n",
        "    #forward pass \n",
        "    yhat = X * W + b\n",
        "    \n",
        "    #calcuate the loss (difference squared error)\n",
        "    error = yhat - y\n",
        "    loss = tf.reduce_mean(tf.square(error))\n",
        "  \n",
        "  #evalute the gradient with the respect to the paramters\n",
        "  dW, db = tape.gradient(loss, [W, b])\n",
        "\n",
        "  #update the paramters using Gradient Descent  \n",
        "  W.assign_sub(dW * learning_rate)\n",
        "  b.assign_sub(db* learning_rate)\n",
        "\n",
        "  #print the loss every 20 iterations \n",
        "  if i % 20 == 0:\n",
        "    print(\"Loss at step {:03d}: {:.3f}\".format(i, loss))\n",
        "      \n",
        "print(f'W : {W.numpy()} , b  = {b.numpy()} ')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss at step 000: 14.640\n",
            "Loss at step 020: 6.655\n",
            "Loss at step 040: 3.369\n",
            "Loss at step 060: 2.016\n",
            "Loss at step 080: 1.458\n",
            "Loss at step 100: 1.228\n",
            "Loss at step 120: 1.134\n",
            "Loss at step 140: 1.095\n",
            "Loss at step 160: 1.078\n",
            "Loss at step 180: 1.072\n",
            "W : 2.8997178077697754 , b  = 1.9168860912322998 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HAbR8BM7J3-t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dataset Processing and Augmentation"
      ]
    },
    {
      "metadata": {
        "id": "nLYuN04dPFJd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CIbF-GUHPXZO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can easily use MNIST "
      ]
    },
    {
      "metadata": {
        "id": "fQxLk3_Lk3OP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b7dfbee2-f112-4db5-fab3-0c32f21ef63a"
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ncJvViT2PgQD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can apply a set of transformations using `tf.image`"
      ]
    },
    {
      "metadata": {
        "id": "7WcLqQ4gNao_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "8df8ecae-9611-4096-828b-1a4fa6f0785c"
      },
      "cell_type": "code",
      "source": [
        "img = x_train[2:3]\n",
        "img = np.expand_dims(img, -1)\n",
        "\n",
        "#flipping horizontally\n",
        "img1 = tf.image.flip_left_right(img)\n",
        "\n",
        "#flipping vertically\n",
        "img2 = tf.image.flip_up_down(img)\n",
        "\n",
        "\n",
        "plt.axis('off')\n",
        "plt.imshow(tf.concat([img, img1, img2], axis = 2).numpy().squeeze())\n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAC1CAYAAAAeGPXQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAACZpJREFUeJzt3U+I1HUfB/BZjTKzKGNBNiQql1wo\nEYNEMZNsDSI66M0OSQfxol0kFMGDsITgIfSyehA8ZATVISIo3TAKyiAQFXYtIUIMYck8WNkfnefy\nEM/D9zPPM+vMfGZ/M6/X8c3vN/Pd7zTz7gcff7+Ber1erwEAaeZ0ewEA0G+ULwAkU74AkEz5AkAy\n5QsAyZQvACRTvgCQTPkCQDLlCwDJlC8AJFO+AJBM+QJAMuULAMmULwAkU74AkEz5AkAy5QsAyZQv\nACRTvgCQTPkCQDLlCwDJlC8AJFO+AJDsjm4vAID+MTExEeajo6NFtnv37iIbGxtr+5q6wZUvACRT\nvgCQTPkCQDLlCwDJDFwBkOb9998P83q9nryS7nLlCwDJlC8AJFO+AJBM+QJAMuULAMlMO/OPycnJ\nMH/++eeL7MyZM0U2ODjY9jXNBtPT00W2fPnyIjt58mR4/sjISNvXBL1mYGCg20tI5coXAJIpXwBI\npnwBIJnyBYBklR24+v7778P8l19+KbKnn36608vpCadPnw7z9evXJ69k9ov2pNH+GbhqzjfffFNk\nDzzwQHjs8PBwp5fDLPDcc891ewkd48oXAJIpXwBIpnwBIJnyBYBklR24mpiYCPOpqakiM3BVip6d\n2WiI7bvvvuv0cion2pPFixeHx0Z73W9382nG8ePHi2zp0qXhsQauek/0PZkzp3evD3v3LwOAWUr5\nAkAy5QsAyZQvACRTvgCQrLLTzgcPHgzzDRs2JK+kmq5fv15kb775Znjs66+/XmS9+uzeSPS3rlq1\nqsga7d+uXbuK7N577219YX2g0fd827ZtySuh06J/AfDwww93YSU5XPkCQDLlCwDJlC8AJFO+AJCs\nsgNXN2/e7PYSKm0mAyueR1uayZ5Ee/3222+3czk9y/ecXuXKFwCSKV8ASKZ8ASCZ8gWAZJUYuPrp\np5+K7PLly11YSe+4evVq08eOjo52cCXVNJM9mcle898afc+j34ShoaFOL4dkH3zwQZHt3LmzCytp\nP1e+AJBM+QJAMuULAMmULwAkU74AkKwS086ffvppkf32229dWEk1/frrr0V27ty5ps9/8MEH27mc\nnjCTPYn2OvpM7rnnnpbW1Isafc+j34QtW7Z0eDUzd+zYsTA/depUyvtv3rw5zJcsWVJkjzzySKeX\nU6vVarWXXnopzMfHx4vs6NGjRWbaGQC4LcoXAJIpXwBIpnwBIFklBq7Onz/f9LHLly/v4Eqqac+e\nPUUW3Z5v2bJl4fl33nln29dUddGeNNq/s2fPFln0mbz11lutL6zCZvLdnclvQpa9e/cW2djYWHjs\nwoULi+zxxx9v6f2/+uqrIms08HXHHeVP/4IFC4ps5cqVRbZmzZrwNZ955pkiiz7TF154ITx/0aJF\nRXbx4sXw2F7gyhcAkilfAEimfAEgmfIFgGSVGLiaiWhAoOr++OOPIvv222+L7MiRI+H57777blPv\nc/DgwTCfN29eU+f3k2hPGu3funXriuzw4cNFdu3atfD8rVu3FtlTTz1VZHfddVd4flVU/bsb3Y2r\nXq+Hx3744YdFtmrVqpbePxqivHXrVnjs559/XmQ//PBDkf35559FduDAgfA19+/fX2TRXds2bNgQ\nnv/7778XWbT+6enp8PzBwcEwn61c+QJAMuULAMmULwAkU74AkKznBq4aDa20ImuQ4dChQ+Fr3rx5\ns8hmMsgQDQf99ddfRTYyMhKeT3Ma7d+cOeX/40afSaPhnE2bNhVZ9EjCuXPnFtn27dvD14zu0BU9\nUu7ZZ58Nz4/+pqGhofDYZnXiuztb3X///W1/zZns/yuvvHLb77Nv374wb3Uw9MaNG0UW/c5OTk6G\n5xu4AgD+J+ULAMmULwAkU74AkEz5AkCySkw7z58/v8gGBgbCY19++eUi68RzMhtNprbynMydO3eG\nr9nsczKjCeharVZbvHhxkUXTslWbFpxtGu1fNBl89913F9nRo0fD86PP6syZM0X2xRdfFNmXX34Z\nvubp06eL7Pr160X2999/h+dH379Wb4944cKFpt6nVot/E6ok2v+q/2uD6Pamq1evbiqr1eIJ8OiW\nrTt27AjP//rrr4tsNt8a15UvACRTvgCQTPkCQDLlCwDJKjFwFd3O7LHHHguPPXXqVNvff3h4uMg2\nb94cHrtkyZIii27b1wkff/xxmF+5cqXIli5d2unl8G/RfxNTU1NF9sknn4Tnv/jii0W2du3aprKZ\niG6DevHixfDY48ePt/Rekeh7Fj0LuVar1V599dW2v3+rnnjiiaaPjQbm+t3Y2FiRvffee0V29uzZ\n8Pzolr0GrgCAfyhfAEimfAEgmfIFgGSVGLiKNBq4mI2DGFk++uijpo997bXXOrgS/lO012+88UaR\nNfr8ooGrTogGAxsNC46OjnZ6OZUTPU+76nfiyhTdoe/JJ58ssuj56rVarfbzzz8X2X333df6wjrE\nlS8AJFO+AJBM+QJAMuULAMmULwAkq+y0M63ZuHFjt5fQN6K9jqadqbahoaEie+ihh7qwkt6xcOHC\npo89ceJEkW3durWdy2krV74AkEz5AkAy5QsAyZQvACQzcAXQIXPnzu32EiptfHy8yN55553w2MnJ\nyU4vp61c+QJAMuULAMmULwAkU74AkMzAVR+o1+tF9uOPPxbZo48+mrGcvhPtdfSZ0Ht27NgR5lNT\nU8krqaYFCxYU2e7du8NjJyYmimx6errIBgcHW19YG7jyBYBkyhcAkilfAEimfAEgmfIFgGSmnfvA\nwMBAkd26dasLK+lP0V5Hnwm9Z/369WG+YsWK5JVUU/Q9GR4eDo+9dOlSp5fTVq58ASCZ8gWAZMoX\nAJIpXwBIZuCqT3322WdF1mg4hNZEe01/aDQcxO1buXJlmO/Zsyd5Ja1x5QsAyZQvACRTvgCQTPkC\nQDIDV33As2NnH58J3J6RkZEwv3z5cvJKWuPKFwCSKV8ASKZ8ASCZ8gWAZMoXAJKZdu4hmzZtCvPx\n8fHklfD/RM8pbfT5Ab3HlS8AJFO+AJBM+QJAMuULAMkG6u5zBwCpXPkCQDLlCwDJlC8AJFO+AJBM\n+QJAMuULAMmULwAkU74AkEz5AkAy5QsAyZQvACRTvgCQTPkCQDLlCwDJlC8AJFO+AJBM+QJAMuUL\nAMmULwAkU74AkEz5AkAy5QsAyZQvACRTvgCQTPkCQDLlCwDJlC8AJFO+AJBM+QJAMuULAMmULwAk\nU74AkEz5AkAy5QsAyZQvACRTvgCQTPkCQDLlCwDJlC8AJPsXJuzd509fWUIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "C8oJ-u_UP_eQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we examplain the pipeline of creating a dataset. Note that we have 3 main important functions in `tf.data.Dataset `\n",
        "\n",
        "\n",
        "\n",
        "*   `from_tensor_slices` which convert the input to a tensor \n",
        "*   `map` which applies a certain function to every tensor \n",
        "* `batch` which creates a batch of tensors of certain size\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Y2CeekFxkMXc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def agument(x):\n",
        "  #apply random flipping horizontally and vertically \n",
        "  x = tf.image.random_flip_left_right(x)\n",
        "  x = tf.image.random_flip_up_down(x)\n",
        "  return x\n",
        "\n",
        "def pre_process(x, y):\n",
        "  #normalize and expand\n",
        "  x = tf.cast(x, tf.float32)/255.\n",
        "  x = tf.expand_dims(x, -1)\n",
        "  \n",
        "  #cast the labels\n",
        "  y = tf.cast(y, tf.int32)\n",
        "  return x, y\n",
        "\n",
        "def create_dataset(x, y, training = True):  \n",
        "  #convert to tensors and shuffle\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((x , y)).shuffle(len(x)-1)\n",
        "  \n",
        "  #extract batches\n",
        "  dataset = dataset.batch(32)\n",
        "  \n",
        "  #preprocess the batch\n",
        "  dataset = dataset.map(pre_process, num_parallel_calls = 4)\n",
        "  \n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0ywOcraLRFg0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can create a dataset and iterate over it "
      ]
    },
    {
      "metadata": {
        "id": "s8kGBjCblM3K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dataset = create_dataset(x_train, y_train, training = True)\n",
        "test_dataset  = create_dataset(x_test , y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OOZ576C4IsUV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "649cb292-5008-4175-c75f-7a40cb56a715"
      },
      "cell_type": "code",
      "source": [
        "for x, y in train_dataset:\n",
        "  plt.imshow(x[0].numpy().squeeze())\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "  break"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCP4BTyMKha3oD3YAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "JE8BnpyA7kfZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# End-to-End CNN Example\n",
        "\n",
        "Here we create a simple convolutional neural network (CNN)to recognize hand-written digits (MNIST). We start by creating a simple alexnet CNN model. \n",
        "\n",
        "![alt text](https://i.imgur.com/twUKrlo.png)"
      ]
    },
    {
      "metadata": {
        "id": "5nJYdULqiee7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Convolution2D, MaxPooling2D, Flatten, BatchNormalization, Dropout\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_cuZWAmejXH4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "  model = Sequential()\n",
        "  model.add(Convolution2D(filters = 16, kernel_size = 3, padding = 'same', input_shape = [28, 28, 1], activation = 'relu'))\n",
        "  model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "  model.add(Convolution2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
        "  model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(units = 100, activation = 'relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(units = 10 , activation = 'softmax'))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YkEgDGCmO7_i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create the model "
      ]
    },
    {
      "metadata": {
        "id": "LvTw6rrnkiMI",
        "colab_type": "code",
        "outputId": "99c57bbd-e61d-45ba-e3f6-0300e55fb4d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        }
      },
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "model.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 16)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1568)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               156900    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 162,710\n",
            "Trainable params: 162,710\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w8xMs3nh5cw2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Look at the output by frowarding a batch of zero images. "
      ]
    },
    {
      "metadata": {
        "id": "ZilarnvZ5cDS",
        "colab_type": "code",
        "outputId": "be4d089b-5358-4bc9-bdc1-b389e16cc3c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "model(np.zeros((10, 28, 28, 1), np.float32))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=8706, shape=(10, 10), dtype=float32, numpy=\n",
              "array([[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
              "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
              "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
              "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
              "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
              "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
              "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
              "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
              "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
              "       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "nIRZj1pyPnbJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Design the loss function, Gradient and Accuracy metric"
      ]
    },
    {
      "metadata": {
        "id": "6ERHfE-Z4KvV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#evaluate the loss\n",
        "def loss(y, yhat):\n",
        "  #this applies the loss to sparse labels i.e not one hot encoded \n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(y, yhat)\n",
        "\n",
        "#record the gradient with respect to the model variables \n",
        "def grad(model, x, y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    yhat = model(x)\n",
        "    loss_value = loss(y, yhat)\n",
        "  return tape.gradient(loss_value, model.trainable_variables)\n",
        "\n",
        "#calcuate the accuracy of the model \n",
        "def accuracy(y, yhat):\n",
        "  \n",
        "  #get the labels of the predicted values \n",
        "  yhat = tf.argmax(yhat, 1).numpy()\n",
        "  \n",
        "  #get the labels of the true values\n",
        "  y    = y.numpy()\n",
        "  return np.sum(y == yhat)/len(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yw-Mk_nJP0vp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Intitalize the variables"
      ]
    },
    {
      "metadata": {
        "id": "6UXs1mEKSRwe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#use Adam optimizer \n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "#record epoch loss and accuracy  \n",
        "loss_history = tf.keras.metrics.Mean(\"loss\")\n",
        "accuracy_history = tf.keras.metrics.Mean(\"accuracy\")\n",
        "\n",
        "#epochs\n",
        "epochs = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xm5mBuUOSQwg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "id": "XUx2dlTirg_t",
        "colab_type": "code",
        "outputId": "122b1501-e0e3-4c12-9da0-059cac45b290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "  for x, y in train_dataset:\n",
        "    \n",
        "    yhat = model(x)\n",
        "    # Calculate derivatives of the input function with respect to its parameters.\n",
        "    grads = grad(model, x, y)\n",
        "\n",
        "    # Apply the gradient to the model\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    #record the current loss and accuracy   \n",
        "    loss_history(loss(y, yhat))\n",
        "    accuracy_history(accuracy(y, yhat))\n",
        "\n",
        "  print(\"epoch: {:d} Loss: {:.3f}, Acc: {:.3f}\".format(epoch, loss_history.result(), accuracy_history.result()))\n",
        "\n",
        "  #clear the history \n",
        "  loss_history.reset_states()\n",
        "  accuracy_history.reset_states()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1 Loss: 0.145, Acc: 0.956\n",
            "epoch: 2 Loss: 0.047, Acc: 0.985\n",
            "epoch: 3 Loss: 0.031, Acc: 0.990\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "onYJgb64Yct-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ]
    },
    {
      "metadata": {
        "id": "Xdk9lG-sB6Y5",
        "colab_type": "code",
        "outputId": "07985abd-3817-4da4-8328-318ccfa55078",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "accuracy_history.reset_states()\n",
        "for x, y in test_dataset:\n",
        "    # Calculate derivatives of the input function with respect to its parameters.\n",
        "    yhat = model(x)\n",
        "\n",
        "    #record the current loss and accuracy   \n",
        "    accuracy_history(accuracy(y, yhat))\n",
        "print(\"Acc: {:.2f}\".format(accuracy_history.result()))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc: 0.99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VKtzhgMYNxFr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Save and Restore a Model\n",
        "You can save training history then restore it later. "
      ]
    },
    {
      "metadata": {
        "id": "Ci3mmDTLM7oI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#create a directory for saving the model\n",
        "import os\n",
        "\n",
        "#check if it exists\n",
        "if os.path.isdir('model'):\n",
        "  raise Exception('Foulder exists !')\n",
        "else:\n",
        "  checkpoint_dir = 'model'\n",
        "  os.mkdir(checkpoint_dir)\n",
        "\n",
        "  #create a root for the checkpoint\n",
        "  checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "  root = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                             model=model)\n",
        "\n",
        "  #save the model \n",
        "  root.save(file_prefix=checkpoint_prefix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SMkTMzQjUjYy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Restore the model "
      ]
    },
    {
      "metadata": {
        "id": "AHFYo3Z-UPG0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#create an empty model \n",
        "model = create_model()\n",
        "\n",
        "#create a checkpoint variable \n",
        "root = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                           model=create_model())\n",
        "\n",
        "#restore the model\n",
        "root.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "#retrieve the trained model \n",
        "model = root.model "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}